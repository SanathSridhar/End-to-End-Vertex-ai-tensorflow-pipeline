{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25e6d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"Your-Project-ID\"\n",
    "\n",
    "# Set the project id of your GCP Project\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d202767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the region and uri of your google cloud bucket\n",
    "\n",
    "REGION = \"Your-Region\"\n",
    "BUCKET_URI = f\"gs://Your-Bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c1219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your GCP service account\n",
    "\n",
    "SERVICE_ACCOUNT = \"Your-Service-Account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9290b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow your service account to access your cloud bucket\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58107584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, InputPath,OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676b1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init your project\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193e1d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n",
      "PIPELINE_ROOT: gs://imperial-data-306906-test-bucket-unique/tf_pipeline\n"
     ]
    }
   ],
   "source": [
    "#Define environment for vertex ai and set your pipeline root\n",
    "\n",
    "PATH = %env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/tf_pipeline\"  # This is where all pipeline artifacts are sent. You'll need to ensure the bucket is created ahead of time\n",
    "PIPELINE_ROOT\n",
    "print(f\"PIPELINE_ROOT: {PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16777868",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.7', packages_to_install=['numpy==1.21.0', 'google-cloud-storage==2.11.0', 'pandas==1.2.4', 'tensorflow==2.9.3'])\n",
    "def get_data(message: str,\n",
    "             x_train_path: OutputPath(Artifact),\n",
    "             y_train_path: OutputPath(Artifact),\n",
    "             x_test_path: OutputPath(Artifact),\n",
    "             y_test_path: OutputPath(Artifact)) -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"output_message\", str),\n",
    "        (\"dataset_details\", str)\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets and splits the MNIST dataset using TensorFlow, saving datasets to GCS.\n",
    "\n",
    "    Args:\n",
    "        message (str): A message for logging.\n",
    "        x_train_path (OutputPath): Path to save the training images.\n",
    "        y_train_path (OutputPath): Path to save the training labels.\n",
    "        x_test_path (OutputPath): Path to save the testing images.\n",
    "        y_test_path (OutputPath): Path to save the testing labels.\n",
    "\n",
    "    Returns:\n",
    "        NamedTuple: Outputs containing message and dataset details.\n",
    "    \"\"\"\n",
    "\n",
    "     from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    (x_train_split, y_train_split), (x_test_split, y_test_split) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Save the split datasets to specified paths\n",
    "    np.save(x_train_path, x_train_split)\n",
    "    np.save(x_test_path, x_test_split)\n",
    "    np.save(y_train_path, y_train_split)\n",
    "    np.save(y_test_path, y_test_split)\n",
    "\n",
    "    # Calculate and store dataset details\n",
    "    dataset_details_dict = {\n",
    "        \"Num Train Images\": str(x_train_split.shape[0]),\n",
    "        \"Num Test Images\": str(x_test_split.shape[0])\n",
    "    }\n",
    "\n",
    "    # Create output message and details\n",
    "    output_message = \"Split Data into Test and Train\"\n",
    "    output_message = f\"Message: {output_message}\"\n",
    "    dataset_details = f\"Split Details:\\n{dataset_details_dict}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5669ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 2: Normalize Data\n",
    "@component(base_image='python:3.7', packages_to_install=['numpy==1.21.0', 'google-cloud-storage==2.11.0', 'pandas==1.2.4'])\n",
    "def normalize_data(message: str,\n",
    "                 x_dataset_path: InputPath(Artifact),\n",
    "                 x_dataset_reshaped_path: OutputPath(Artifact)\n",
    "):\n",
    "    \"\"\"\n",
    "    Normalizes and reshapes the input data.\n",
    "\n",
    "    Args:\n",
    "        message (str): A message for logging.\n",
    "        x_dataset_path (InputPath): Path to the original dataset.\n",
    "        x_dataset_reshaped_path (OutputPath): Path to save the normalized and reshaped dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import io\n",
    "\n",
    "    # Load the original dataset from x_dataset_path\n",
    "    # Add file extension .npy\n",
    "\n",
    "    x_dataset_path = f\"{x_dataset_path}.npy\"\n",
    "    x = np.load(x_dataset_path)\n",
    "\n",
    "    # Reshape and Normalize the images\n",
    "    x = x.reshape(-1,28,28,1)\n",
    "    x_reshaped = x / 255\n",
    "\n",
    "    # Save the reshaped data as in the x_dataset_reshaped path\n",
    "    np.save(x_dataset_reshaped_path , x_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3868c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 3: Train Model\n",
    "@component(base_image='python:3.7', packages_to_install=['numpy==1.21.0','google-cloud-storage==2.11.0','pandas==1.2.4','tensorflow==2.9.3','joblib==1.1.0'])\n",
    "def train_model(x_train_path: InputPath(Artifact),\n",
    "                y_train_path: InputPath(Artifact),\n",
    "                x_test_path: InputPath(Artifact),\n",
    "                y_test_path: InputPath(Artifact),\n",
    "                message: str,\n",
    "                trained_model: Output[Model],\n",
    "                metrics: Output[Metrics],\n",
    "                no_epochs:int = 5,\n",
    "                optimizer: str = \"adam\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds, trains, and evaluates a TensorFlow model.\n",
    "\n",
    "    Args:\n",
    "        x_train_path (InputPath): Path to the normalized and reshaped training images.\n",
    "        y_train_path (InputPath): Path to the training labels.\n",
    "        x_test_path (InputPath): Path to the normalized and reshaped testing images.\n",
    "        y_test_path (InputPath): Path to the testing labels.\n",
    "        message (str): A message for logging.\n",
    "        trained_model (Output[Model]): Output for the trained model artifact.\n",
    "        metrics (Output[Metrics]): Output for tracking metrics.\n",
    "        no_epochs (int): Number of training epochs.\n",
    "        optimizer (str): Optimizer for model training.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    # Define and configure the neural network model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))  # Output has 10 classes, numbers from 0-9\n",
    "\n",
    "    # Get model summary for logging\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "\n",
    "    # Compile the model with specified optimizer and loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Load Training Data\n",
    "    x_train_path = f\"{x_train_path}.npy\"\n",
    "    x_train = np.load(x_train_path)\n",
    "\n",
    "    y_train_path = f\"{y_train_path}.npy\"\n",
    "    y_train = np.load(y_train_path)\n",
    "\n",
    "    # Train the model and return the training history\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=no_epochs,\n",
    "        batch_size=20 \n",
    "    )\n",
    "\n",
    "    # Load Test Data\n",
    "    x_test_path = f\"{x_test_path}.npy\"\n",
    "    x_test = np.load(x_test_path)\n",
    "\n",
    "    y_test_path = f\"{y_test_path}.npy\"\n",
    "    y_test = np.load(y_test_path)\n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss and accuracy value for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test, y=y_test)\n",
    "\n",
    "    # Log accuracy metric\n",
    "    metrics.log_metric(\"Accuracy\", (float(model_accuracy) * 100.0))\n",
    "\n",
    "    # Log loss metric\n",
    "    metrics.log_metric(\"Loss\", float(model_loss))\n",
    "\n",
    "    # Generate output predictions for the input samples\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions, axis=1)  # The prediction outputs 10 values, take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # Create directories for saving the trained model\n",
    "    os.makedirs(trained_model.path, exist_ok=True)\n",
    "    model_dir = trained_model.path\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    tf.saved_model.save(model, model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d82b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 4: Deploy TensorFlow Model\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],)\n",
    "def deploy_tensorflow_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploys a TensorFlow model to Vertex AI for serving.\n",
    "\n",
    "    Args:\n",
    "        model (Input[Model]): Input for the trained model.\n",
    "        project_id (str): Google Cloud Project ID.\n",
    "        vertex_endpoint (Output[Artifact]): Output for the deployed model endpoint.\n",
    "        vertex_model (Output[Model]): Output for the deployed model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    # Upload the TensorFlow model to Vertex AI\n",
    "    model_display_name = \"tf-mnist-classifier-model\"  # Provide a unique name for your model\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-4:latest\",  # Use the appropriate TensorFlow version and CPU/GPU image\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    # Update the Vertex AI endpoint and model outputs\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12612785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Kubeflow Pipeline named \"tf-pipeline\"\n",
    "@dsl.pipeline(\n",
    "    name=\"tf-pipeline\",\n",
    ")\n",
    "def pipeline():\n",
    "    # Step 1: Get Data\n",
    "    get_data_task = get_data(\n",
    "        message='Getting MNIST Dataset From TensorFlow'\n",
    "    )\n",
    "    \n",
    "    # Step 2: Normalize Data for Training\n",
    "    normalize_data_train_task = normalize_data(\n",
    "        x_dataset_path=get_data_task.outputs['x_train_path'],\n",
    "        message='Reshaping Images in Train Set'\n",
    "    )\n",
    "    \n",
    "    # Step 3: Normalize Data for Testing\n",
    "    normalize_data_test_task = normalize_data(\n",
    "        x_dataset_path=get_data_task.outputs['x_test_path'],\n",
    "        message='Reshaping Images in Test Set'\n",
    "    )\n",
    "    \n",
    "    # Step 4: Train TensorFlow Model\n",
    "    train_model_task = train_model(\n",
    "        x_train_path=normalize_data_train_task.outputs['x_dataset_reshaped_path'],\n",
    "        y_train_path=get_data_task.outputs['y_train_path'],\n",
    "        x_test_path=normalize_data_test_task.outputs['x_dataset_reshaped_path'],\n",
    "        y_test_path=get_data_task.outputs['y_test_path'],\n",
    "        message='Training TensorFlow Model'\n",
    "    )\n",
    "    \n",
    "    # Step 5: Deploy TensorFlow Model to Vertex AI\n",
    "    deploy_tensorflow_model_task = deploy_tensorflow_model(\n",
    "        model=train_model_task.outputs['trained_model'],\n",
    "        project_id=PROJECT_ID\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752d7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Kubeflow Pipeline using the provided pipeline function\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"tf-pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61436dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tf-pipeline-20231110212520?project=138999697430\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/138999697430/locations/us-central1/pipelineJobs/tf-pipeline-20231110212520\n"
     ]
    }
   ],
   "source": [
    "# Create a Vertex AI PipelineJob for the compiled pipeline\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"tf-pipeline\",           # Set a display name for the pipeline job\n",
    "    template_path=\"tf-pipeline.yaml\",     # Path to the compiled pipeline YAML file\n",
    "    pipeline_root=PIPELINE_ROOT,          # Root directory where pipeline artifacts will be stored\n",
    ")\n",
    "\n",
    "# Run the compiled pipeline using Vertex AI PipelineJob\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
